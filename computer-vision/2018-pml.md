---
description: Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning
---

# \(2018\) PML

* 논문링크: [https://arxiv.org/pdf/1804.03131.pdf](https://arxiv.org/pdf/1804.03131.pdf)
* 참조자료: [https://eungbean.github.io/2019/09/10/pml/](https://eungbean.github.io/2019/09/10/pml/)



1. Introduction

* we formulate video object segmentation as pixel-wise retrieval in a learned embedding space.
* 이상적으로, 임베딩 공간에서, 같은 object instance에 속해있는 픽셀은 가깝고, 다른 object 간의 픽셀은 멀리 떨어져 있다.
* 우리는 이런 임베딩 공간을 VOS를 위해 수정된 triplet loss를 사용하여, FCN으로 학습한다. 
* 픽셀 간의 분명한 상응관계는 주어지지 않는다. 
* 임베딩 모델이 학습되면, 각 프레임에 대해서 임베딩 벡터를 추출하고, 가장 유사한 annotated pixel을 찾기 위해, per-pixel nearest neighbor search 를 수행한다.
* 사융자의 annotation에 의한 object는 비디오 시퀀스전반에 걸처 segment 될 수 있다.

2. Related Work

Semi-Supervised and Unsupervised Video Object Segmentation:

* Semi-supervised VOS:
  * first frame을 입력으로 segmentation mask를 얻는다.
  * MaskTrack: 이전 프레임으로 부터 현재 프레임의 segmentation을 전파하여 얻는다.
  * OSVOS: 첫번째 프레임의 표현을 FCN으로 학습하고, 나머지 프레임의 segmentation을 병렬적으로 수행한다.
  * ...
* Unsupervised VOS:
  * 비디오만 입력으로 사용한다.
  * ...

Interactive Video Object Segmentation:

Deep Metric Learning:





3. Proposed Method

3.1. Overview

![](../.gitbook/assets/image%20%2844%29.png)

* Our method에서 새로운 비디오를 처리과정은 2단계로 구성된다.
  * 1단계: 각 필섹을 d 차원의  공간에 임베딩을한다.
  * 2단계: 각 픽셀별 검색\(retrieval\)을 수행하여, 인접 참조 픽셀의 라벨로 예측한다.

3.2. Segmentation as Pixel-wise Retrieval

* 문제를 단순화하기 위해,  single-object detection으로 ....
* semi-supervised VOS의 Task: 
  * 첫 프레임의 object 마스크가 주어지면, 나머지 프레임의 segmentation을 수행하는 것.
  * 라벨\(l\)이 0이면, 배경이고, 1이면 우리가 관심있는 object이다.
  * 이러한 annotation된 픽셀을 reference pixels로 참조한다.
  * 목표는 라벨링되지않은 다른 프레임의 라벨을 추론하는 것이다.
* 
![i-th &#xD53D;&#xC140;, j-th &#xD504;&#xB808;&#xC784;](../.gitbook/assets/image%20%2853%29.png)

![user&#xAC00; &#xC81C;&#xACF5;&#xD558;&#xB294; &#xCCAB;&#xBC88;&#xC9F8; &#xD504;&#xB808;&#xC784;&#xC758; annotation. ](../.gitbook/assets/image%20%2821%29.png)

Embedding Model:

* 임베딩을 위해 ResNet101기반 DeepLab-v2모델을 백본으로 하여 feature extractor를 학습.
* base feature extractor:
  * ResNet101기반 DeepLab-v2모델을 백본으로 사용
* embedding head:
  * d-차원으로 임베딩하기 위한 컨볼루션 레이어\(DeepLab-v2에서 final layer를 제거하고\)

![](../.gitbook/assets/image%20%28112%29.png)

* 입력이미지 크기가 h x w일 경우, output은 \[h/8, w/8, d\]이다.
* d는 임베딩 차원으로 보통 128을 사용한다.



* 임베딩에 FCN을 이용하면, 공간정보와 시간정보를 손실하게 된다. 
* 이러한 이슈를 해결하기 위해, embedding head에 공간 좌표 정보와 frame number를 추가 입력으로 넣어 주었다.
  * i가 좌표 정보이고, j가 프레임 정보이다.

![](../.gitbook/assets/image%20%28120%29.png)

Retrieval with Online Adaptation:

* inference시, VOS는 임베딩 공간에서 reference 픽셀가 가장 가까운 픽셀을 검색하여 수행된다.
* 이때, k-Nearest Neighbors \(kNN\) 분류기를 사용하였다.
* 실험에서, semi-supervised case인 경우에 k를 5개로, interactive segmentation case인 경우에는 k를 1로 하였다.



* semi-supervised VOS의 major challenge는 비디오 frame에 따라 표현\(appearance\)이 바뀐다는 점이다.
* ...
* 비디오가 진행됨에 따라, 참조 샘플 pool에 신뢰도가 높은\(k=5근처의 레이블이 모두 일치\)샘플을 추가한다.

3.3. Training

![anchor sample](../.gitbook/assets/image%20%2868%29.png)

![positive sample \(from positive sample pool P\)](../.gitbook/assets/image%20%2828%29.png)

![negative sample](../.gitbook/assets/image%20%2832%29.png)



* 모든 positive point를 가깝게 학습하고 싶지 않다.
* 왜나면, object의 다른 부분은 다르게 보이기 때문이다.
* 따라서 Loss를 수정하여,  smallest negative points를 smallest positive points 보다 더 멀리 밀어낸다.

![&#xC6D0;&#xB798; triplet loss](../.gitbook/assets/image%20%2847%29.png)

![&#xC218;&#xC815;&#xB41C; triplet loss](../.gitbook/assets/image%20%2810%29.png)



Training Strategy:

* 학습에는 모든 프레임에 온전히 라벨이 달린 비디오를 사용한다.
* traning video에서 3개의 프레임을 랜덤샘플링한다.
* 1개 프레임에서는 anchor 포인트를 샘플링하고, 나머지 2개 프레임에서 anchor 포인트와 같은 라벨을 가지면, positive pool에 다르면, negative pool에 넣는다.
* Note that the pools are sampled from two different frames to have temporal variety, which is needed for the embedding head to learn to weight the temporal information from the feature vector. 
* Also, we do not use pixels from the the anchor frame in the pools to avoid too easy samples.
* In each iteration, a forward pass is performed on three randomly selected frames with one frame as the anchor. 
* Then the anchor frame is used to sample 256 anchor samples, and the positive and negative pools are all foreground and background pixels in the other two frames. 
* We compute the loss according to Equation 1 and the network is trained in an end to end manner.

